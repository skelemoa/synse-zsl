{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import os.path as osp\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "import torchvision.models as models\n",
    "from resnext_specialist import VA\n",
    "from data_cnn60 import NTUDataLoaders, AverageMeter, make_dir, get_cases, get_num_classes\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='View adaptive')\n",
    "# parser.add_argument('--ss', type=int, help=\"split size\")\n",
    "# parser.add_argument('--st', type=str, help=\"split type\")\n",
    "# parser.add_argument('--dataset', type=str, help=\"dataset path\")\n",
    "# parser.add_argument('--wdir', type=str, help=\"directory to save weights path\")\n",
    "# parser.add_argument('--le', type=str, help=\"language embedding model\")\n",
    "# parser.add_argument('--ve', type=str, help=\"visual embedding model\")\n",
    "# parser.add_argument('--phase', type=str, help=\"train or val\")\n",
    "# parser.add_argument('--gpu', type=str, help=\"gpu device number\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "gpu = '0'\n",
    "ss = 5\n",
    "st = 'r'\n",
    "dataset_path = 'ntu_results/shift_val_5_r'\n",
    "wdir = 'pos_aware_cada_vae_concatenated_latent_space_shift_5_r_val'\n",
    "le = 'bert_large'\n",
    "ve = 'shift'\n",
    "phase = 'val'\n",
    "num_classes = 60\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "seed = 5\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "device = torch.device(\"cuda\")\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "criterion2 = nn.MSELoss().to(device)\n",
    "\n",
    "if ve == 'vacnn':\n",
    "    vis_emb_input_size = 2048\n",
    "elif ve == 'shift':\n",
    "    vis_emb_input_size = 256\n",
    "else: \n",
    "    pass    \n",
    "    \n",
    "text_hidden_size = 100\n",
    "vis_hidden_size = 512\n",
    "output_size = 50\n",
    "\n",
    "if le == 'bert_large':\n",
    "    noun_emb_input_size = 1024\n",
    "    verb_emb_input_size = 1024\n",
    "elif le == 'w2v':\n",
    "    noun_emb_input_size = 300\n",
    "    verb_emb_input_size = 300\n",
    "else:\n",
    "    pass\n",
    "\n",
    "ntu_loaders = NTUDataLoaders(dataset_path, 'max', 1)\n",
    "train_loader = ntu_loaders.get_train_loader(1024, 8)\n",
    "zsl_loader = ntu_loaders.get_val_loader(1024, 8)\n",
    "val_loader = ntu_loaders.get_test_loader(1024, 8)\n",
    "zsl_out_loader = ntu_loaders.get_val_out_loader(1024, 8)\n",
    "val_out_loader = ntu_loaders.get_test_out_loader(1024, 8)\n",
    "train_size = ntu_loaders.get_train_size()\n",
    "zsl_size = ntu_loaders.get_val_size()\n",
    "val_size = ntu_loaders.get_test_size()\n",
    "print('Train on %d samples, validate on %d samples' % (train_size, zsl_size))\n",
    "\n",
    "\n",
    "nouns_vocab = np.load('nouns_vocab.npy')\n",
    "verbs_vocab = np.load('verbs_vocab.npy')\n",
    "nouns = nouns_vocab[np.argmax(np.load('nouns_ohe.npy'), -1)][:num_classes]\n",
    "# nouns[nouns == 'object'] = '#'\n",
    "# nouns[51] = 'someone'\n",
    "# nouns[55] = 'object'\n",
    "# nouns = np.load('nouns.npy')\n",
    "verbs = verbs_vocab[np.argmax(np.load('verbs_ohe.npy'), -1)][:num_classes]\n",
    "# nouns = np.load('nouns.npy')\n",
    "# verbs = np.load('verbs.npy')\n",
    "# prps = np.load('prepositions.npy')\n",
    "labels = np.load('labels.npy')\n",
    "\n",
    "if phase == 'val':\n",
    "    gzsl_inds = np.load('./label_splits/'+ st + 's' + str(num_classes - ss) +'.npy')\n",
    "    unseen_inds = np.sort(np.load('./label_splits/' + st + 'v' + str(ss) + '_0.npy'))\n",
    "    seen_inds = np.load('./label_splits/'+ st + 's' + str(num_classes - ss -ss) + '_0.npy')\n",
    "else:\n",
    "    gzsl_inds = np.arange(60)\n",
    "    unseen_inds = np.sort(np.load('./label_splits/' + st + 'u' + str(ss) + '.npy'))\n",
    "    seen_inds = np.load('./label_splits/'+ st + 's' + str(num_classes - ss) + '.npy')\n",
    "\n",
    "unseen_labels = labels[unseen_inds]\n",
    "seen_labels = labels[seen_inds]\n",
    "\n",
    "unseen_nouns = nouns[unseen_inds]\n",
    "unseen_verbs = verbs[unseen_inds]\n",
    "# unseen_prps = prps[unseen_inds]\n",
    "seen_nouns = nouns[seen_inds]\n",
    "seen_verbs = verbs[seen_inds]\n",
    "# seen_prps = prps[seen_inds]\n",
    "verb_corp = np.unique(verbs[gzsl_inds])\n",
    "noun_corp = np.unique(nouns[gzsl_inds])\n",
    "# prp_corp = np.unique(prps[gzsl_inds])\n",
    "\n",
    "# import gensim\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('/ssd_scratch/cvit/pranay.gupta/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "\n",
    "# def get_w2v(model, words):\n",
    "#     emb = np.zeros([300])\n",
    "#     for word in words.split():\n",
    "#         emb += model[word]\n",
    "#     emb /= len(words.split())\n",
    "    \n",
    "#     return emb\n",
    "\n",
    "\n",
    "verb_emb = torch.from_numpy(np.load(le + '_verb.npy')[:num_classes, :]).view([num_classes, verb_emb_input_size])\n",
    "verb_emb = verb_emb/torch.norm(verb_emb, dim = 1).view([num_classes, 1]).repeat([1, verb_emb_input_size])\n",
    "noun_emb = torch.from_numpy(np.load(le + '_noun.npy')[:num_classes, :]).view([num_classes, noun_emb_input_size])\n",
    "noun_emb = noun_emb/torch.norm(noun_emb, dim = 1).view([num_classes, 1]).repeat([1, noun_emb_input_size])\n",
    "# prp_w2v = torch.from_numpy(np.array([get_w2v(model, i) for i in prps])).view([60, 300])\n",
    "# prp_w2v = noun_emb/torch.norm(prp_w2v, dim = 1).view([60, 1]).repeat([1, 300])\n",
    "\n",
    "unseen_verb_emb = verb_emb[unseen_inds, :]\n",
    "unseen_noun_emb = noun_emb[unseen_inds, :]\n",
    "# unseen_prp_w2v = prp_w2v[unseen_inds, :]\n",
    "\n",
    "seen_verb_emb = verb_emb[seen_inds, :]\n",
    "seen_noun_emb = noun_emb[seen_inds, :]\n",
    "# seen_prp_w2v = prp_w2v[seen_inds, :]\n",
    "print(\"loaded language embeddings\")\n",
    "\n",
    "\n",
    "def get_text_data(target, verb_emb, noun_emb):\n",
    "    return verb_emb[target].view(target.shape[0], verb_emb_input_size).float(), noun_emb[target].view(target.shape[0], verb_emb_input_size).float()\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar', is_best=False):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "        \n",
    "def accuracy(class_embedding, vis_trans_out, target, inds):\n",
    "    inds = torch.from_numpy(inds).to(device)\n",
    "    temp_vis = vis_trans_out.unsqueeze(1).expand(vis_trans_out.shape[0], class_embedding.shape[0], vis_trans_out.shape[1])\n",
    "    temp_cemb = class_embedding.unsqueeze(0).expand(vis_trans_out.shape[0], class_embedding.shape[0], vis_trans_out.shape[1])\n",
    "    preds = torch.argmax(torch.sum(temp_vis*temp_cemb, axis=2), axis = 1)\n",
    "    acc = torch.sum(inds[preds] == target).item()/(preds.shape[0])\n",
    "    return acc, torch.sum(temp_vis*temp_cemb, axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "unseen_zs = np.load('../../tf_vaegan_test/12_12_r_unseen_zs.npy')\n",
    "seen_zs = np.load('../../tf_vaegan_test/12_12_r_seen_zs.npy')\n",
    "unseen_train = np.load('../../synse_resources/ntu_results/shift_val_12_r/ztest_out.npy')\n",
    "seen_train = np.load('../../synse_resources/ntu_results/shift_val_12_r/val_out.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_scale(seen_features, T):\n",
    "    return np.array([np.exp(i)/np.sum(np.exp(i)) for i in (seen_features + 1e-12)/T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh 0.73\n",
      "0.8085714285714286\n",
      "0.8238071570576541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "for f in [12]:\n",
    "    print(f)\n",
    "    for t in [5]:\n",
    "        print(t)\n",
    "        fin_val_acc = 0\n",
    "        fin_train_acc = 0\n",
    "        for run in range(1):\n",
    "            prob_unseen_zs = unseen_zs\n",
    "#             prob_unseen_zs = np.array([np.exp(i)/np.sum(np.exp(i)) for i in unseen_zs])\n",
    "    #         prob_noun_unseen_zs = np.array([np.exp(i)/np.sum(np.exp(i)) for i in noun_unseen_zs])\n",
    "    #         prob_verb_unseen_zs = np.array([np.exp(i)/np.sum(np.exp(i)) for i in verb_unseen_zs])\n",
    "            prob_unseen_train = temp_scale(unseen_train, t)\n",
    "#             prob_unseen_train = np.array([np.exp(i)/np.sum(np.exp(i)) for i in unseen_train])\n",
    "        #     np.array([np.exp(i)/np.sum(np.exp(i)) for i in unseen_train])\n",
    "            prob_seen_zs = seen_zs\n",
    "#             prob_seen_zs = np.array([np.exp(i)/np.sum(np.exp(i)) for i in seen_zs])\n",
    "    #         prob_noun_seen_zs = np.array([np.exp(i)/np.sum(np.exp(i)) for i in noun_seen_zs])\n",
    "    #         prob_verb_seen_zs = np.array([np.exp(i)/np.sum(np.exp(i)) for i in verb_seen_zs])\n",
    "            prob_seen_train = temp_scale(seen_train, t)\n",
    "#             prob_seen_train = np.array([np.exp(i)/np.sum(np.exp(i)) for i in seen_train])\n",
    "        #     np.array([np.exp(i)/np.sum(np.exp(i)) for i in seen_train])\n",
    "\n",
    "            feat_unseen_zs = np.sort(prob_unseen_zs, 1)[:,::-1][:,:f]\n",
    "    #         feat_noun_unseen_zs = np.sort(prob_noun_unseen_zs, 1)[:,::-1]\n",
    "    #         feat_verb_unseen_zs = np.sort(prob_verb_unseen_zs, 1)[:,::-1]\n",
    "            feat_unseen_train = np.sort(prob_unseen_train, 1)[:,::-1][:,:f]\n",
    "            feat_seen_zs = np.sort(prob_seen_zs, 1)[:,::-1][:,:f]\n",
    "    #         feat_noun_seen_zs = np.sort(prob_noun_seen_zs, 1)[:,::-1]\n",
    "    #         feat_verb_seen_zs = np.sort(prob_verb_seen_zs, 1)[:,::-1]\n",
    "            feat_seen_train = np.sort(prob_seen_train, 1)[:,::-1][:,:f]\n",
    "\n",
    "            val_unseen_inds = np.random.choice(np.arange(feat_unseen_train.shape[0]), 300, replace=False)\n",
    "            val_seen_inds = np.random.choice(np.arange(feat_seen_train.shape[0]), 400, replace=False)\n",
    "            train_unseen_inds = np.array(list(set(list(np.arange(feat_unseen_train.shape[0]))) - set(list(val_unseen_inds))))\n",
    "            train_seen_inds = np.array(list(set(list(np.arange(feat_seen_train.shape[0]))) - set(list(val_seen_inds))))\n",
    "\n",
    "    #         mod_unseen_zs_feat = np.concatenate([feat_noun_unseen_zs, feat_verb_unseen_zs], 1)\n",
    "    #         mod_seen_zs_feat = np.concatenate([feat_noun_seen_zs, feat_verb_seen_zs], 1)\n",
    "            gating_train_x = np.concatenate([np.concatenate([feat_unseen_zs[train_unseen_inds, :], feat_unseen_train[train_unseen_inds, :]], 1), np.concatenate([feat_seen_zs[train_seen_inds, :], feat_seen_train[train_seen_inds, :]], 1)], 0)\n",
    "            gating_train_y = [0]*len(train_unseen_inds) + [1]*len(train_seen_inds)\n",
    "            gating_val_x = np.concatenate([np.concatenate([feat_unseen_zs[val_unseen_inds, :], feat_unseen_train[val_unseen_inds, :]], 1), np.concatenate([feat_seen_zs[val_seen_inds, :], feat_seen_train[val_seen_inds, :]], 1)], 0)\n",
    "            gating_val_y = [0]*len(val_unseen_inds) + [1]*len(val_seen_inds)\n",
    "\n",
    "            train_inds = np.arange(gating_train_x.shape[0])\n",
    "            np.random.shuffle(train_inds)\n",
    "        #     val_inds = np.arange(gating_val_x.shape[0])\n",
    "        #     np.random.shuffle(val_inds)\n",
    "            model = LogisticRegression(random_state=0, C=1, solver='lbfgs', n_jobs=-1,\n",
    "                                         multi_class='multinomial', verbose=1, max_iter=5000,\n",
    "                                         ).fit(gating_train_x[train_inds, :], np.array(gating_train_y)[train_inds])\n",
    "            prob = model.predict_proba(gating_val_x)\n",
    "            best = 0\n",
    "            bestT = 0\n",
    "            for t in range(25, 75, 1):\n",
    "                y = prob[:, 0] > t/100\n",
    "                acc = np.sum((1 - y) == gating_val_y)/len(gating_val_y)\n",
    "#                 print(acc)\n",
    "                if acc > best:\n",
    "                    best = acc\n",
    "                    bestT = t/100\n",
    "            fin_val_acc += best\n",
    "            pred_train = model.predict(gating_train_x)\n",
    "            train_acc = np.sum(pred_train == gating_train_y)/len(gating_train_y)\n",
    "            fin_train_acc += train_acc\n",
    "        print('thresh', bestT)\n",
    "        print(fin_val_acc/1)\n",
    "        print(fin_train_acc/1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('../../tf_vaegan_test/gating_model_t5_thresh0.74_seen.pkl', 'wb') as f:\n",
    "    pkl.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = model.predict_proba(gating_val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best = 0\n",
    "bestT = 0\n",
    "for t in range(25, 75, 1):\n",
    "    y = prob[:, 0] > t/100\n",
    "    acc = np.sum((1 - y) == gating_val_y)/len(gating_val_y)\n",
    "    print(acc)\n",
    "    if acc > best:\n",
    "        best = acc\n",
    "        bestT = t/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
